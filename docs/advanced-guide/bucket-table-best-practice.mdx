---
title: Bucket table best practice
tags:
  - Docs
---

Bucket table Description
When using Bucket table in ByConity, the system will organize the table data according to one or more columns and expressions provided by the user in the table creation statement, and cluster the data with the same value under the same bucket number.

Benefits of using Bucket table
The following benefits can be obtained on large tables by using cluster key to aggregate data:

Point checks for cluster keys can filter out most of the data, reduce the amount of IO to obtain shorter execution time and higher concurrent QPS
For aggregate calculation of cluster key, computing nodes can pre-calculate data subsets to achieve smaller memory footprint and shorter execution time
The join of two or more tables for the cluster key can be optimized by co-located join, which greatly reduces the amount of shuffle data and results in shorter execution time
When to Consider Bucket Tables
The table is large enough that the number of parts under a partition needs to be at least significantly more than the number of workers
Query statements can benefit from the above benefits
How to choose a cluster key
The Cluster key can be one or more columns and expressions. It is recommended to use up to 3 fields. More fields usually introduce high write costs and the scope of benefit statements is smaller.

Choosing the correct cluster key has a significant impact on performance, so it needs to be chosen carefully. Generally, the following principles can be followed:

Columns often used for equality, IN filtering
Commonly used aggregation columns
Multi-table join key
If the above scenario is commonly used in combination of two columns, such as a = 1 and b = 2, then cluster key can get better results by selecting two columns.

Another dimension to consider is the number of distinct values for a column, given that

Data with the same cluster key will belong to the same bucket number
All parts under a bucket number will be sent to the same worker for calculation
We can conclude that

In order to utilize all worker nodes for computation, the value of distinct needs to exceed at least the number of workers
If the number of distinct values is small, the largest distinct value is preferred, preferably a multiple of the number of workers, to achieve a more balanced data distribution during query
For example:

In the table test, the commonly used filter columns are c1, c2, c3, and the columns are independent of each other
The number of workers is 30
distinct c1 is 6
distinct c2 is 8
distinct c3 is 5
It can be seen that the individual distinct values of the three columns are all smaller than the number of workers. Disticnt c1, c2 is 48. Although it is greater than the number of workers, it is not a multiple of the number of workers, so it is not a suitable cluster key. The value of distinct c1, c3 is 30, which is exactly 1 times the number of workers, but considering the balance of data distribution, it is more appropriate to choose a larger value of distinct c1, c2, c3 to be 8 times the number of workers

How to choose the bucket number
Given that within a partition

Each row of data will be calculated according to the value of the cluster key as % BUCKETS to get the corresponding bucket number
Parts of the same bucket number will be sent to the same worker node for calculation when querying
Therefore, choosing an appropriate bucket number has a major impact on storage and query, and generally has the following principles:

Make sure that the bucket number is a multiple of the number of workers. This is to ensure the balance of the query allocation. It is generally recommended to set it to 1 or 2 times (reserve to expand the redundancy of worker nodes). The number of worker nodes is enough
Make sure that there is enough data in a bucket number under a partition, and do not generate too small parts. Therefore, it is recommended that if the table is relatively small, at least ensure that the size of a bucket number part of a partition exceeds 1GB. Do not set too high a bucket nubmer, a bucket number smaller than the number of workers can appear
How to decide whether to modify the cluster by definition
During operation, due to data changes, query mode changes, and changes in the number of worker nodes, users may want to reset the cluster key and bucket number.

Here you need to consider the cost of implementing the modification, and weigh whether you need to modify and when:

Modifying the cluster by definition requires reclustering existing data, and evaluating the amount of existing data to estimate recluster execution time
During the recluster period, the query of the existing data will fall back to a common table, and all the benefits of the bucket table will be temporarily lost
Recluster will occupy the resources of the write worker. It is necessary to evaluate whether the current cnch cluster has an independent write worker and the current load, and evaluate the impact on tasks such as existing queries and merges.
There are two cases of modifying the cluster by definition:

modify cluster key
At this time, it means that the current data can no longer obtain the bucket table income, so there is no need to consider the lost income during recluster
It is necessary to evaluate the impact of the recluster task on existing tasks to determine whether it can be executed
Modify the bucket number
The income of the current bucket table is still there, so it is necessary to confirm with the business side the acceptable performance rollback time, further judge whether it can be executed according to the recluster time, and determine the start execution time
It is also necessary to evaluate the impact of the recluster task on existing tasks to determine whether it can be executed
